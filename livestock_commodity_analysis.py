# -*- coding: utf-8 -*-
"""Livestock Commodity Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KJkImvsEKzY05R-BzusyFMRfKEd0Ib8b

# Importing Section

## Importing Libraries
"""

#Importing Pandas Library for manipulating the dataset
import pandas as pd
 #Importing numpy to perform numerical operatioons
import numpy as np
#Importing plotly for visualization
import plotly.express as px
#Importing train test split
from sklearn.model_selection import train_test_split
#Importing Linear Regression and its evaluation methods
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""## Mounting into Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""## Importing files

"""

#importing all the files as csv
df1 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /horseasses-population-in-nepal-by-district.csv')
df2 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /milk-animals-and-milk-production-in-nepal-by-district.csv')
df3 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /net-meat-production-in-nepal-by-district.csv')
df4 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /production-of-cotton-in-nepal-by-district.csv')
df5 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /production-of-egg-in-nepal-by-district.csv')
df6 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /rabbit-population-in-nepal-by-district.csv')
df7 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /wool-production-in-nepal-by-district.csv')
df8 = pd.read_csv('/content/drive/MyDrive/data analysis and machine learning /yak-nak-chauri-population-in-nepal-by-district.csv')

"""# Data Description of raw datasets

## Finding the basic information
"""

#checking the first five datas of all dataset
dfs = [df1, df2, df3, df4, df5, df6, df7, df8]
for i, df in enumerate(dfs, start=1):
    print(f"df{i}:")
    display(df.head(5))

#checking the last five datas of all dataset
dfs = [df1, df2, df3, df4, df5, df6, df7, df8]
for i, df in enumerate(dfs, start=1):
    print(f"df{i}:")
    display(df.tail(5))

#checking the shape of all the datasets
dfs = [df1, df2, df3, df4, df5, df6, df7, df8]
for i, df in enumerate(dfs, start =1):
  print(f"df{i}")
  display(df.shape)

"""## Checking for null values"""

#checking the null values present in the datasets
dfs = [df1, df2, df3, df4, df5, df6, df7, df8]
for i, df in enumerate (dfs, start=1):
  missing_values = df.isnull().sum()
  print(missing_values)

"""We can see that there is one missing value present in the raw dataset but since we are going to merge all the datasets I decided to handle it after merging it.

# Merging all the datasets
"""

dfs = [df1, df2, df3, df4, df5, df6, df7, df8]
# Merge dataframes using outer join
for df in dfs:
    df['DISTRICT'] = df['DISTRICT'].str.upper()
merged_df = pd.merge(dfs[0], dfs[1], on='DISTRICT', how='outer')
for df in dfs[2:]:
    merged_df = pd.merge(merged_df, df, on='DISTRICT', how='outer')

"""# Data description of merged dataset

## Finding Basic Information
"""

#checking the first ten data of the merged data
merged_df.head(10)

#checking the last ten data of the merged data
merged_df.tail(10)

merged_df.shape

merged_df.info()

"""# Handling missing values

## Finding out missing values
"""

#checking the null values present in the merged dataset
merged_df.isnull().sum()

#checking the total missing values present in the merged dataset
merged_df.isnull().sum().sum()

"""## Replacing the missing values"""

#Replacing the missing values with 0
new_df = merged_df.fillna(value =0)

"""# Data Description of new datset

## Finding Basic Information of new
"""

#Looking for the information of the new datset to understanf the data better
new_df.info()

"""## Converting certain Data Types"""

#Checking the first 10 data of the new dataset
new_df.head(10)

#Checking the last 10 data of the new dataset
new_df.tail(10)

#Converting certain floats into int
columns_to_convert = ['Horses/Asses','MILKING  COWS NO.', 'MILKING  BUFFALOES NO.', 'LAYING HEN', 'LAYING DUCK', 'HEN EGG', 'DUCK EGG', 'TOTAL EGG', 'Rabbit', 'SHEEPS NO.', 'YAK/NAK/CHAURI']
new_df[columns_to_convert] = new_df[columns_to_convert].astype(int)

"""## Checking the converted DType"""

#Checking the new converted information
new_df.info()

#Checking the numerical description of the dataset
new_df.describe()

"""# Looking for Duplicated Values"""

#Checking for the duplicated values in the dataset
new_df.duplicated()

#checking the total number of duplicated values
new_df.duplicated().sum()

"""Since there are no duplicated values there is no need to perform any data cleaning operations.

# Looking for Extreme/Unrealistic values
"""

for col in new_df.columns:
    unique_values = new_df[col].unique()
    print(f"Unique values in column '{col}': ")
    print(f"{col} : => {unique_values}\n")
    print(f"------------------------------------------------------------------------------------")

"""## Dropping wrong districts

We are dropping the rows which are not considered as district.
"""

new_df = new_df.drop([13,18,34,50,58,59,60,63,66,67,71,80,85,86,87,89,90,91,92,93,94,95,96,97,100,101,102])
new_df

"""## Renaming the name of districts"""

district_mapping = {
    'TERHATHUM': 'TERATHUM',
    'SANKHUWASHAVA': 'SANKHUWASABHA',
    'RAMECHAP': 'RAMECHHAP'
}

# Replace values in DISTRICT column according to the mapping
new_df['DISTRICT'] = new_df['DISTRICT'].replace(district_mapping)
new_df

"""## Merging rows"""

# Merging rows based on the DISTRICT column
new_df = new_df.groupby('DISTRICT').sum().reset_index()
new_df

"""## Looking for more unrealistic data like negative values


"""

numreric_columns = new_df.select_dtypes(include='int').columns
new_df[numreric_columns].lt(0).any(axis=1).sum()

"""Since there are no negative values or unrealistic values present, there is no need to look further into it."""

new_df.to_csv('/content/drive/MyDrive/data analysis and machine learning /Individual Assignment /cleaned_dataset.csv', index = False)

"""# Correlation

## Finding correlation of features with Total Milk Produced
"""

correlation_matrix = new_df.corr()
totalMilkProduced_correlation = correlation_matrix['TOTAL MILK PRODUCED'].sort_values(ascending = False)
print(totalMilkProduced_correlation)

"""## Plotting Heatmap"""

fig = px.imshow(correlation_matrix['TOTAL MILK PRODUCED'].values.reshape(1, -1),
                labels=dict(color="Correlation"),
                x=correlation_matrix.columns,
                y=['TOTAL MILK PRODUCED'],
                color_continuous_scale='RdBu',
                color_continuous_midpoint=0)

# Updating the layout for better visualization
fig.update_layout(title='Correlation Heatmap of TOTAL MILK PRODUCED',
                  xaxis_title='Features',
                  yaxis_title='TOTAL MILK PRODUCED',
                  width=800,
                  height=600)

fig.show()

"""# Visualization"""

new_df.describe()

"""## Box Plot of Total Milk Produced"""

fig1 = px.box(new_df, y ="TOTAL MILK PRODUCED", title="Boxplot of Total Milk Produced")
fig1.show()

"""## Scatter Plot of Buff Milk"""

fig2 = px.scatter(new_df, x="MILKING  BUFFALOES NO.", y ="BUFF MILK", title="Scatter of Buff Milk", trendline='ols')
fig2.show()

"""## Pie chart of Milk Produced by Animal Type"""

# Calculating total milk produced by cows and buffaloes
total_milk_by_animal_type = new_df[['COW MILK', 'BUFF MILK']].sum()

# Creating a DataFrame from the total milk production by animal type
total_milk_df = pd.DataFrame({'Animal Type': ['Cows', 'Buffaloes'], 'Total Milk Production': total_milk_by_animal_type})

# Create the pie chart using Plotly Express
fig = px.pie(total_milk_df, values='Total Milk Production', names='Animal Type', title='Pie Chart of Milk Production by Animal Type')

# Show the pie chart
fig.show()

"""## Bar Graph of Total Milk Produced by Animal Type"""

# Calculate=ing total milk produced by cows and buffaloes
cows = new_df['COW MILK'].sum()
buff = new_df['BUFF MILK'].sum()

# Creating a DataFrame for the milk production by animal type
data = {'Animal Type': ['Cows', 'Buffaloes'],
        'Total Milk Produced': [cows, buff]}
new_df_animal_type = pd.DataFrame(data)

# Creating the bar chart using Plotly Express
fig = px.bar(new_df_animal_type, x='Animal Type', y='Total Milk Produced',
             color='Animal Type', color_discrete_map={'Cows': 'pink', 'Buffaloes': 'grey'},
             title='Bar Chart of Milk Produced by Animal Type')

# Show the plot
fig.show()

"""# Train Test Split"""

X = new_df[['MILKING  COWS NO.',	'MILKING  BUFFALOES NO.',	'COW MILK',	'BUFF MILK',]]
y = new_df[['TOTAL MILK PRODUCED']]

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Linear Regression

## Model Training Linear Regression
"""

# Creating and training the linear regression model
regression = LinearRegression()
regression.fit(X_train, y_train)

"""## Linear Regression Model Prediction"""

#Making Predictions on the testing data
y_pred_linear = regression.predict(X_test)
y_pred_linear

"""## Linear Regression Model Evaluation"""

#Evaluating the model
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_linear)
print("Mean Squared Error:", mse)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print("Root Mean Squared Error:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_linear)
print("Mean Absolute Error:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_linear)
print("R-squared (R2):", r2)